{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lplab/anaconda3/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data: [1, 2, 3, 4, 5]\n",
      "Squared Data:  [1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Create a Spark configuration\n",
    "conf = SparkConf().setAppName(\"SquareIntegers\").setMaster(\"local[*]\")\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Sample data: Replace this with your set of integers\n",
    "data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Parallelize the data into an RDD (Resilient Distributed Dataset)\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Use map transformation to square each element\n",
    "squared_rdd = rdd.map(lambda x: x**2)\n",
    "\n",
    "# Collect the results back to the driver program\n",
    "result = squared_rdd.collect()\n",
    "\n",
    "# Print the squared values\n",
    "print(\"Original Data: {}\".format(data))\n",
    "print(\"Squared Data:  {}\".format(result))\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lplab/anaconda3/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data: [10, 5, 8, 15, 3]\n",
      "Maximum Number: 15\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Create a Spark configuration\n",
    "conf = SparkConf().setAppName(\"MaxOfNumbers\").setMaster(\"local[*]\")\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Sample data: Replace this with your set of numbers\n",
    "data = [10, 5, 8, 15, 3]\n",
    "\n",
    "# Parallelize the data into an RDD\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Use the reduce action to find the maximum\n",
    "max_number = rdd.reduce(lambda x, y: max(x, y))\n",
    "\n",
    "# Print the maximum number\n",
    "print(\"Original Data: {}\".format(data))\n",
    "print(\"Maximum Number: {}\".format(max_number))\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lplab/anaconda3/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data: [10, 5, 8, 15, 3]\n",
      "Total Sum: 41\n",
      "Count: 5\n",
      "Average: 8.2\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Create a Spark configuration\n",
    "conf = SparkConf().setAppName(\"AverageOfNumbers\").setMaster(\"local[*]\")\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Sample data: Replace this with your set of numbers\n",
    "data = [10, 5, 8, 15, 3]\n",
    "\n",
    "# Parallelize the data into an RDD\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Use the reduce action to calculate the sum\n",
    "total_sum = rdd.reduce(lambda x, y: x + y)\n",
    "\n",
    "# Calculate the average\n",
    "count = rdd.count()\n",
    "average = total_sum / count if count > 0 else 0\n",
    "\n",
    "# Print the results\n",
    "print(\"Original Data: {}\".format(data))\n",
    "print(\"Total Sum: {}\".format(total_sum))\n",
    "print(\"Count: {}\".format(count))\n",
    "print(\"Average: {}\".format(average))\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lplab/anaconda3/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+------------+--------------------+-----+-----------------+\n",
      "|year|industry_code_ANZSIC|industry_name_ANZSIC|rme_size_grp|            variable|value|             unit|\n",
      "+----+--------------------+--------------------+------------+--------------------+-----+-----------------+\n",
      "|2011|                   A|Agriculture, Fore...|         a_0|       Activity unit|46134|            COUNT|\n",
      "|2011|                   A|Agriculture, Fore...|         a_0|Rolling mean empl...|    0|            COUNT|\n",
      "|2011|                   A|Agriculture, Fore...|         a_0|Salaries and wage...|  279|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|         a_0|Sales, government...| 8187|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|         a_0|        Total income| 8866|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|         a_0|   Total expenditure| 7618|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|         a_0|Operating profit ...|  770|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|         a_0|        Total assets|55700|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|         a_0|Fixed tangible as...|32155|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|       b_1-5|       Activity unit|21777|            COUNT|\n",
      "|2011|                   A|Agriculture, Fore...|       b_1-5|Rolling mean empl...|38136|            COUNT|\n",
      "|2011|                   A|Agriculture, Fore...|       b_1-5|Salaries and wage...| 1435|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|       b_1-5|Sales, government...|13359|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|       b_1-5|        Total income|13771|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|       b_1-5|   Total expenditure|12316|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|       b_1-5|Operating profit ...| 1247|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|       b_1-5|        Total assets|52666|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|       b_1-5|Fixed tangible as...|31235|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|       c_6-9|       Activity unit| 1965|            COUNT|\n",
      "|2011|                   A|Agriculture, Fore...|       c_6-9|Rolling mean empl...|13848|            COUNT|\n",
      "+----+--------------------+--------------------+------------+--------------------+-----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ReadCSV\").getOrCreate()\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "csv_file_path = \"Data.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lplab/anaconda3/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the DataFrame:\n",
      "+----+--------------------+--------------------+------------+--------------------+-----+-----------------+\n",
      "|year|industry_code_ANZSIC|industry_name_ANZSIC|rme_size_grp|            variable|value|             unit|\n",
      "+----+--------------------+--------------------+------------+--------------------+-----+-----------------+\n",
      "|2011|                   A|Agriculture, Fore...|         a_0|       Activity unit|46134|            COUNT|\n",
      "|2011|                   A|Agriculture, Fore...|         a_0|Rolling mean empl...|    0|            COUNT|\n",
      "|2011|                   A|Agriculture, Fore...|         a_0|Salaries and wage...|  279|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|         a_0|Sales, government...| 8187|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|         a_0|        Total income| 8866|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|         a_0|   Total expenditure| 7618|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|         a_0|Operating profit ...|  770|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|         a_0|        Total assets|55700|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|         a_0|Fixed tangible as...|32155|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|       b_1-5|       Activity unit|21777|            COUNT|\n",
      "|2011|                   A|Agriculture, Fore...|       b_1-5|Rolling mean empl...|38136|            COUNT|\n",
      "|2011|                   A|Agriculture, Fore...|       b_1-5|Salaries and wage...| 1435|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|       b_1-5|Sales, government...|13359|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|       b_1-5|        Total income|13771|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|       b_1-5|   Total expenditure|12316|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|       b_1-5|Operating profit ...| 1247|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|       b_1-5|        Total assets|52666|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|       b_1-5|Fixed tangible as...|31235|DOLLARS(millions)|\n",
      "|2011|                   A|Agriculture, Fore...|       c_6-9|       Activity unit| 1965|            COUNT|\n",
      "|2011|                   A|Agriculture, Fore...|       c_6-9|Rolling mean empl...|13848|            COUNT|\n",
      "+----+--------------------+--------------------+------------+--------------------+-----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "DataFrame Schema:\n",
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- industry_code_ANZSIC: string (nullable = true)\n",
      " |-- industry_name_ANZSIC: string (nullable = true)\n",
      " |-- rme_size_grp: string (nullable = true)\n",
      " |-- variable: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- unit: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"DisplayDataFrame\").getOrCreate()\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "csv_file_path = \"Data.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"First few rows of the DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# Display the schema of the DataFrame\n",
    "print(\"DataFrame Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lplab/anaconda3/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for 'value' column:\n",
      "+-------+------------------+\n",
      "|summary|value             |\n",
      "+-------+------------------+\n",
      "|count  |17028             |\n",
      "|mean   |12858.970234173803|\n",
      "|stddev |83791.06859238271 |\n",
      "|min    |-1                |\n",
      "|max    |C                 |\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ColumnSummaryStatistics\").getOrCreate()\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "csv_file_path = \"Data.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Specify the column for which you want to calculate summary statistics\n",
    "selected_column = \"value\"\n",
    "\n",
    "# Calculate summary statistics for the selected column\n",
    "column_summary = df.select(selected_column).describe()\n",
    "\n",
    "# Display the summary statistics\n",
    "print(\"Summary Statistics for '{}' column:\".format(selected_column))\n",
    "column_summary.show(truncate=False)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
