{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2af0473-20cf-4097-a541-09dd1114e0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image= tensor([[0.3644, 0.3050, 0.9278, 0.1052, 0.6828, 0.5182],\n",
      "        [0.1996, 0.1654, 0.3114, 0.0051, 0.5672, 0.7513],\n",
      "        [0.7540, 0.1651, 0.5668, 0.6994, 0.7762, 0.3250],\n",
      "        [0.8696, 0.7020, 0.4507, 0.2992, 0.8078, 0.2905],\n",
      "        [0.2020, 0.1711, 0.2923, 0.2807, 0.0608, 0.0678],\n",
      "        [0.3181, 0.0740, 0.4829, 0.9101, 0.3511, 0.3971]])\n",
      "image.shape= torch.Size([1, 6, 6])\n",
      "image.shape= torch.Size([1, 1, 6, 6])\n",
      "image= tensor([[[[0.3644, 0.3050, 0.9278, 0.1052, 0.6828, 0.5182],\n",
      "          [0.1996, 0.1654, 0.3114, 0.0051, 0.5672, 0.7513],\n",
      "          [0.7540, 0.1651, 0.5668, 0.6994, 0.7762, 0.3250],\n",
      "          [0.8696, 0.7020, 0.4507, 0.2992, 0.8078, 0.2905],\n",
      "          [0.2020, 0.1711, 0.2923, 0.2807, 0.0608, 0.0678],\n",
      "          [0.3181, 0.0740, 0.4829, 0.9101, 0.3511, 0.3971]]]])\n",
      "kernel= tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "outimage= tensor([[[[3.7596, 3.2514, 4.6421, 4.4306],\n",
      "          [4.1846, 3.3652, 4.4840, 4.5219],\n",
      "          [4.1735, 3.6273, 4.2340, 3.6076],\n",
      "          [3.5626, 3.6630, 3.9356, 3.4651]]]])\n"
     ]
    }
   ],
   "source": [
    "# Q1\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "image = torch.rand(6,6)\n",
    "print(\"image=\", image)\n",
    "#Add a new dimension along 0th dimension\n",
    "#i.e. (6,6) becomes (1,6,6). This is because\n",
    "#pytorch expects the input to conv2D as 4d tensor\n",
    "image = image.unsqueeze(dim=0)\n",
    "print(\"image.shape=\", image.shape)\n",
    "image = image.unsqueeze(dim=0)\n",
    "print(\"image.shape=\", image.shape)\n",
    "print(\"image=\", image)\n",
    "kernel = torch.ones(3,3)\n",
    "#kernel = torch.rand(3,3)\n",
    "print(\"kernel=\", kernel)\n",
    "kernel = kernel.unsqueeze(dim=0)\n",
    "kernel = kernel.unsqueeze(dim=0)\n",
    "#Perform the convolution\n",
    "outimage = F.conv2d(image, kernel, stride=1, padding=0)\n",
    "print(\"outimage=\", outimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e54f9765-1f34-4431-9bcf-5510c6e3b13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape with torch.nn.Conv2d: torch.Size([1, 3, 4, 4])\n",
      "Output shape with torch.nn.functional.conv2d: torch.Size([1, 3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Q2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the input image\n",
    "image = torch.rand(1, 1, 6, 6)  # Shape: (batch_size, channels, height, width)\n",
    "\n",
    "# Define the Conv2d layer\n",
    "conv_layer = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3)\n",
    "\n",
    "# Apply the Conv2d layer to the input image\n",
    "output = conv_layer(image)\n",
    "\n",
    "print(\"Output shape with torch.nn.Conv2d:\", output.shape)\n",
    "# Define the input image\n",
    "image = torch.rand(1, 1, 6, 6)  # Shape: (batch_size, channels, height, width)\n",
    "\n",
    "# Define the kernel\n",
    "kernel = torch.ones(3, 1, 3, 3)  # Shape: (out_channels, in_channels, kernel_height, kernel_width)\n",
    "\n",
    "# Apply the conv2d operation using torch.nn.functional.conv2d\n",
    "output = F.conv2d(image, kernel, stride=1, padding=0)\n",
    "\n",
    "print(\"Output shape with torch.nn.functional.conv2d:\", output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62685dc5-7d9f-4749-9aa0-5f572dd3c903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d56aec-e1b7-4e77-91f8-f73fcd5403cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc18ab5f-8b31-4ca9-ab35-f17a150cf538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.953\n",
      "[1,   200] loss: 0.218\n",
      "[1,   300] loss: 0.146\n",
      "[1,   400] loss: 0.120\n",
      "[1,   500] loss: 0.103\n",
      "[1,   600] loss: 0.097\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define CNN Classifier\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2), stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2), stride=2)\n",
    "        )\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(128 * 5 * 5, 20),  # Adjusted input size based on the output of the convolutional layers\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.net(x)\n",
    "        return self.classification_head(features.view(x.size(0), -1))\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CNNClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluating the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "predictions = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.numpy())\n",
    "        targets.extend(labels.numpy())\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "# Display confusion matrix\n",
    "conf_matrix = confusion_matrix(targets, predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(10)\n",
    "plt.xticks(tick_marks, np.arange(10))\n",
    "plt.yticks(tick_marks, np.arange(10))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "# Print number of learnable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Total number of learnable parameters: %d' % total_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a37a4-2afd-4d38-b48a-03b7ec49bd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define CNN Classifier with reduced parameters\n",
    "class ReducedCNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReducedCNNClassifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2), stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2), stride=2)\n",
    "        )\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(64 * 5 * 5, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.net(x)\n",
    "        return self.classification_head(features.view(x.size(0), -1))\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:  # Print every 100 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predictions.extend(predicted.numpy())\n",
    "            targets.extend(labels.numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print('Accuracy of the network on the test images: %.2f %%' % accuracy)\n",
    "    return accuracy\n",
    "\n",
    "# Function to plot percentage drop in parameters vs accuracy\n",
    "def plot_drop_vs_accuracy(drop_percentage, accuracies):\n",
    "    plt.plot(drop_percentage, accuracies, marker='o', linestyle='-')\n",
    "    plt.title('Drop in Parameters vs Accuracy')\n",
    "    plt.xlabel('Drop in Parameters (%)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define parameter reduction configurations\n",
    "parameter_reduction_configs = [\n",
    "    {'conv1_channels': 32, 'conv2_channels': 64, 'fc1_out_features': 64},\n",
    "    {'conv1_channels': 16, 'conv2_channels': 32, 'fc1_out_features': 32},\n",
    "    {'conv1_channels': 8, 'conv2_channels': 16, 'fc1_out_features': 16}\n",
    "]\n",
    "\n",
    "# Train and evaluate models with different parameter reduction configurations\n",
    "accuracies = []\n",
    "for config in parameter_reduction_configs:\n",
    "    model = ReducedCNNClassifier()\n",
    "    model.net[0] = nn.Conv2d(1, config['conv1_channels'], kernel_size=3)\n",
    "    model.net[3] = nn.Conv2d(config['conv1_channels'], config['conv2_channels'], kernel_size=3)\n",
    "    model.classification_head[0] = nn.Linear(config['conv2_channels'] * 5 * 5, config['fc1_out_features'])\n",
    "    total_params_before = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    train_model(model, trainloader, criterion, optimizer)\n",
    "    accuracy = evaluate_model(model, testloader)\n",
    "    total_params_after = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    drop_percentage = 100 * (total_params_before - total_params_after) / total_params_before\n",
    "    print(f'Drop in parameters: {drop_percentage:.2f}%')\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Plot drop in parameters vs accuracy\n",
    "drop_percentage = [100 * (1 - total_params_after / total_params_before) for _ in range(len(parameter_reduction_configs))]\n",
    "plot_drop_vs_accuracy(drop_percentage, accuracies)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
