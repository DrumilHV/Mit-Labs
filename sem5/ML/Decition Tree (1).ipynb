{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Outlook': ['sunny','sunny', 'overcast','rainy','rainy','rainy', 'overcast','sunny','sunny','rainy','sunny', 'overcast', 'overcast','rainy'],\n",
    "    'Temp': [85,80,83,70,68,65,64,72,69,75,75,72,81,71],\n",
    "    'Humidity': [85,90,78,96,80,70,65,95,70,80,70,90,75,80],\n",
    "    'Wind': ['weak', 'strong','weak','weak','weak','strong','strong','weak','weak','weak','strong','strong','weak','strong'],\n",
    "    'Decision': ['No','No','yes','yes','yes','no','yes','no','yes','yes','yes','yes','yes','no']    \n",
    "    \n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"DecisionTree.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## ID3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:\n",
      "{'Temp': {85: 'No', 80: 'No', 83: 'yes', 70: 'yes', 68: 'yes', 65: 'no', 64: 'yes', 72: {'Outlook': {'sunny': 'no', 'overcast': 'yes'}}, 69: 'yes', 75: 'yes', 81: 'yes', 71: 'no'}}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Define the dataset\n",
    "data = pd.read_csv('DecisionTree.csv')\n",
    "\n",
    "# Define a function to calculate entropy\n",
    "def entropy(data):\n",
    "    labels = data['Decision']\n",
    "    value_counts = Counter(labels)\n",
    "    total_count = len(labels)\n",
    "    entropy = 0\n",
    "    for label in value_counts:\n",
    "        probability = value_counts[label] / total_count\n",
    "        entropy -= probability * np.log2(probability)\n",
    "#     print(data, \"\\n\",entropy)\n",
    "    return entropy\n",
    "\n",
    "# Define a function to calculate information gain\n",
    "def information_gain(data, attribute):\n",
    "    entropy_before = entropy(data)\n",
    "    unique_values = data[attribute].unique()\n",
    "    weighted_entropy_after = 0\n",
    "    total_count = len(data)\n",
    "\n",
    "    for value in unique_values:\n",
    "        subset = data[data[attribute] == value]\n",
    "        prob_value = len(subset) / total_count\n",
    "        weighted_entropy_after += prob_value * entropy(subset)\n",
    "\n",
    "    return entropy_before - weighted_entropy_after\n",
    "\n",
    "# Define a function to select the best attribute to split on\n",
    "def select_best_attribute(data, attributes):\n",
    "    information_gains = [(attribute, information_gain(data, attribute)) for attribute in attributes]\n",
    "    best_attribute, best_gain = max(information_gains, key=lambda x: x[1])\n",
    "    return best_attribute\n",
    "\n",
    "# Define the ID3 algorithm\n",
    "def id3(data, attributes, parent_data=None):\n",
    "    # If all instances have the same classification, return that classification\n",
    "    if len(np.unique(data['Decision'])) <= 1:\n",
    "        return np.unique(data['Decision'])[0]\n",
    "\n",
    "    # If the dataset is empty, return the classification of the majority class in the parent dataset\n",
    "    elif len(data) == 0:\n",
    "        return np.unique(parent_data['Decision'])[np.argmax(np.unique(parent_data['Decision'], return_counts=True)[1])]\n",
    "\n",
    "    # If there are no more attributes to split on, return the classification of the majority class\n",
    "    elif len(attributes) == 0:\n",
    "        return np.unique(data['Decision'])[np.argmax(np.unique(data['Decision'], return_counts=True)[1])]\n",
    "\n",
    "    else:\n",
    "        best_attribute = select_best_attribute(data, attributes)\n",
    "        tree = {best_attribute: {}}\n",
    "        for value in data[best_attribute].unique():\n",
    "            sub_data = data[data[best_attribute] == value]\n",
    "            sub_attributes = [attr for attr in attributes if attr != best_attribute]\n",
    "            subtree = id3(sub_data, sub_attributes, data)\n",
    "            tree[best_attribute][value] = subtree\n",
    "        return tree\n",
    "\n",
    "# Build the decision tree\n",
    "attributes = data.columns[1:-1]  # Exclude the 'Decision' column\n",
    "decision_tree = id3(data, attributes)\n",
    "\n",
    "# Print the decision tree\n",
    "print(\"Decision Tree:\")\n",
    "print(decision_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.287054028118727\n",
      "0.7506415278096601\n",
      "Index(['Outlook', 'Temp', 'Humidity', 'Wind'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(entropy(data))\n",
    "print(info_gain(data, \"Humidity\"))\n",
    "print(attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4.5 Decision Tree:\n",
      "{'Temp': {85: 'No', 80: 'No', 83: 'yes', 70: 'yes', 68: 'yes', 65: 'no', 64: 'yes', 72: {'Outlook': {'sunny': 'no', 'overcast': 'yes'}}, 69: 'yes', 75: 'yes', 81: 'yes', 71: 'no'}}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Define the dataset\n",
    "data = pd.read_csv('DecisionTree.csv')\n",
    "\n",
    "# Define a function to calculate entropy\n",
    "def entropy(data):\n",
    "    labels = data['Decision']\n",
    "    value_counts = Counter(labels)\n",
    "    total_count = len(labels)\n",
    "    entropy = 0\n",
    "    for label in value_counts:\n",
    "        probability = value_counts[label] / total_count\n",
    "        entropy -= probability * np.log2(probability)\n",
    "    return entropy\n",
    "\n",
    "# Define a function to calculate information gain\n",
    "def information_gain(data, attribute):\n",
    "    entropy_before = entropy(data)\n",
    "    unique_values = data[attribute].unique()\n",
    "    weighted_entropy_after = 0\n",
    "    total_count = len(data)\n",
    "\n",
    "    for value in unique_values:\n",
    "        subset = data[data[attribute] == value]\n",
    "        prob_value = len(subset) / total_count\n",
    "        weighted_entropy_after += prob_value * entropy(subset)\n",
    "\n",
    "    return entropy_before - weighted_entropy_after\n",
    "\n",
    "# Define a function to select the best attribute to split on\n",
    "def select_best_attribute(data, attributes):\n",
    "    information_gains = [(attribute, information_gain(data, attribute)) for attribute in attributes]\n",
    "    best_attribute, best_gain = max(information_gains, key=lambda x: x[1])\n",
    "    return best_attribute\n",
    "\n",
    "# Define the C4.5 algorithm\n",
    "def c45(data, attributes, parent_data=None):\n",
    "    # If all instances have the same classification, return that classification\n",
    "    if len(np.unique(data['Decision'])) <= 1:\n",
    "        return np.unique(data['Decision'])[0]\n",
    "\n",
    "    # If the dataset is empty, return the classification of the majority class in the parent dataset\n",
    "    elif len(data) == 0:\n",
    "        return np.unique(parent_data['Decision'])[np.argmax(np.unique(parent_data['Decision'], return_counts=True)[1])]\n",
    "\n",
    "    # If there are no more attributes to split on, return the classification of the majority class\n",
    "    elif len(attributes) == 0:\n",
    "        return np.unique(data['Decision'])[np.argmax(np.unique(data['Decision'], return_counts=True)[1])]\n",
    "\n",
    "    else:\n",
    "        best_attribute = select_best_attribute(data, attributes)\n",
    "        tree = {best_attribute: {}}\n",
    "        for value in data[best_attribute].unique():\n",
    "            sub_data = data[data[best_attribute] == value]\n",
    "            sub_attributes = [attr for attr in attributes if attr != best_attribute]\n",
    "            subtree = c45(sub_data, sub_attributes, data)\n",
    "            tree[best_attribute][value] = subtree\n",
    "        return tree\n",
    "\n",
    "# Build the C4.5 decision tree\n",
    "attributes = data.columns[1:-1]  # Exclude the 'Decision' column\n",
    "c45_decision_tree = c45(data, attributes)\n",
    "\n",
    "# Print the C4.5 decision tree\n",
    "print(\"C4.5 Decision Tree:\")\n",
    "print(c45_decision_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART Decision Tree:\n",
      "{'attribute': 'Unnamed: 0', 'value': 1.5, 'left': 'No', 'right': {'attribute': 'Temp', 'value': 73.5, 'left': {'attribute': 'Humidity', 'value': 67.5, 'left': 'yes', 'right': 'yes'}, 'right': 'yes'}}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the dataset\n",
    "data = pd.read_csv('DecisionTree.csv')\n",
    "\n",
    "# Filter out non-numeric attributes (excluding 'Decision')\n",
    "attributes = [col for col in data.columns if col != 'Decision' and np.issubdtype(data[col].dtype, np.number)]\n",
    "\n",
    "# Define a function to calculate the Gini impurity\n",
    "def gini_impurity(data):\n",
    "    labels = data['Decision']\n",
    "    total_count = len(labels)\n",
    "    impurity = 1.0\n",
    "    for label in np.unique(labels):\n",
    "        probability = len(labels[labels == label]) / total_count\n",
    "        impurity -= probability**2\n",
    "    return impurity\n",
    "\n",
    "# Define a function to calculate the Gini index for a split\n",
    "def gini_index(data, attribute, value):\n",
    "    left_subset = data[data[attribute] <= value]\n",
    "    right_subset = data[data[attribute] > value]\n",
    "\n",
    "    left_impurity = gini_impurity(left_subset)\n",
    "    right_impurity = gini_impurity(right_subset)\n",
    "\n",
    "    total_count = len(data)\n",
    "    weight_left = len(left_subset) / total_count\n",
    "    weight_right = len(right_subset) / total_count\n",
    "\n",
    "    return weight_left * left_impurity + weight_right * right_impurity\n",
    "\n",
    "# Define a function to select the best attribute and split point\n",
    "def select_best_split(data, attributes):\n",
    "    best_split = None\n",
    "    best_gini = 1.0\n",
    "\n",
    "    for attribute in attributes:\n",
    "        unique_values = data[attribute].unique()\n",
    "        unique_values.sort()\n",
    "\n",
    "        for i in range(1, len(unique_values)):\n",
    "            value = (unique_values[i - 1] + unique_values[i]) / 2\n",
    "            gini = gini_index(data, attribute, value)\n",
    "\n",
    "            if gini < best_gini:\n",
    "                best_gini = gini\n",
    "                best_split = (attribute, value)\n",
    "\n",
    "    return best_split\n",
    "\n",
    "# Define the CART algorithm\n",
    "def cart(data, attributes):\n",
    "    # If all instances have the same classification, return that classification\n",
    "    if len(np.unique(data['Decision'])) == 1:\n",
    "        return np.unique(data['Decision'])[0]\n",
    "\n",
    "    # If there are no more attributes to split on, return the majority class\n",
    "    if len(attributes) == 0:\n",
    "        return np.unique(data['Decision'])[np.argmax(np.unique(data['Decision'], return_counts=True)[1])]\n",
    "\n",
    "    # Otherwise, find the best split and create subtrees\n",
    "    best_split = select_best_split(data, attributes)\n",
    "    best_attribute, best_value = best_split\n",
    "\n",
    "    left_data = data[data[best_attribute] <= best_value]\n",
    "    right_data = data[data[best_attribute] > best_value]\n",
    "\n",
    "    left_subtree = cart(left_data, [attr for attr in attributes if attr != best_attribute])\n",
    "    right_subtree = cart(right_data, [attr for attr in attributes if attr != best_attribute])\n",
    "\n",
    "    return {'attribute': best_attribute, 'value': best_value, 'left': left_subtree, 'right': right_subtree}\n",
    "\n",
    "# Build the CART decision tree\n",
    "cart_decision_tree = cart(data, attributes)\n",
    "\n",
    "# Print the CART decision tree\n",
    "print(\"CART Decision Tree:\")\n",
    "print(cart_decision_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the decision tree structure\n",
    "decision_tree = {'Temp': {85: 'No', 80: 'No', 83: 'yes', 70: 'yes', 68: 'yes', 65: 'no', 64: 'yes', 72: {'Outlook': {'sunny': 'no', 'overcast': 'yes'}}, 69: 'yes', 75: 'yes', 81: 'yes', 71: 'no'}}\n",
    "# global i = 1\n",
    "# Define a function to convert the decision tree to a JSON format\n",
    "def decision_tree_to_json(tree):\n",
    "#     i = 1\n",
    "    if isinstance(tree, dict):\n",
    "        \n",
    "        root_attribute = list(tree.keys())[0]\n",
    "        root_children = tree[root_attribute]\n",
    "        json_tree = {\n",
    "            f\"attribute\": root_attribute,\n",
    "            \"children\": []\n",
    "        }\n",
    "#         i+=1\n",
    "        for value, subtree in root_children.items():\n",
    "            subtree_json = decision_tree_to_json(subtree)\n",
    "            if isinstance(subtree_json, dict):\n",
    "                json_tree[\"children\"].append(subtree_json)\n",
    "            else:\n",
    "                json_tree[\"children\"].append({\"value\": value, \"decision\": subtree})\n",
    "        return json_tree\n",
    "    else:\n",
    "        return tree\n",
    "\n",
    "# Convert the decision tree to JSON format\n",
    "json_decision_tree = decision_tree_to_json(decision_tree)\n",
    "# Define the decision tree structure\n",
    "decision_tree = {'Temp': {85: 'No', 80: 'No', 83: 'yes', 70: 'yes', 68: 'yes', 65: 'no', 64: 'yes', 72: {'Outlook': {'sunny': 'no', 'overcast': 'yes'}}, 69: 'yes', 75: 'yes', 81: 'yes', 71: 'no'}}\n",
    "\n",
    "# Define a function to convert the decision tree to a JSON format\n",
    "def decision_tree_to_json(tree):\n",
    "#     i= 1\n",
    "    if isinstance(tree, dict):\n",
    "        root_attribute = list(tree.keys())[0]\n",
    "        root_children = tree[root_attribute]\n",
    "        json_tree = {\n",
    "            f\"attribute\": root_attribute,\n",
    "            \"children\": []\n",
    "        }\n",
    "#         i+=1\n",
    "        for value, subtree in root_children.items():\n",
    "            subtree_json = decision_tree_to_json(subtree)\n",
    "            if isinstance(subtree_json, dict):\n",
    "                json_tree[\"children\"].append(subtree_json)\n",
    "            else:\n",
    "                json_tree[\"children\"].append({\"value\": value, \"decision\": subtree})\n",
    "        return json_tree\n",
    "    else:\n",
    "        return tree\n",
    "\n",
    "# Convert the decision tree to JSON format\n",
    "json_decision_tree = decision_tree_to_json(decision_tree)\n",
    "\n",
    "# Print the JSON decision tree\n",
    "import json\n",
    "# print(json.dumps(json_decision_tree, indent=2))\n",
    "\n",
    "# Print the JSON decision tree\n",
    "import json\n",
    "# print(json.dumps(json_decision_tree, indent=4))\n",
    "# print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Temp\":\n",
      "    \"85\":\n",
      "            \"No\"\n",
      "    \"80\":\n",
      "            \"No\"\n",
      "    \"83\":\n",
      "            \"yes\"\n",
      "    \"70\":\n",
      "            \"yes\"\n",
      "    \"68\":\n",
      "            \"yes\"\n",
      "    \"65\":\n",
      "            \"no\"\n",
      "    \"64\":\n",
      "            \"yes\"\n",
      "    \"72\":\n",
      "        \"Outlook\":\n",
      "            \"sunny\":\n",
      "                    \"no\"\n",
      "            \"overcast\":\n",
      "                    \"yes\"\n",
      "    \"69\":\n",
      "            \"yes\"\n",
      "    \"75\":\n",
      "            \"yes\"\n",
      "    \"81\":\n",
      "            \"yes\"\n",
      "    \"71\":\n",
      "            \"no\"\n",
      "\n",
      "\"Temp\":\n",
      "    \"85\":\n",
      "            \"No\"\n",
      "    \"80\":\n",
      "            \"No\"\n",
      "    \"83\":\n",
      "            \"yes\"\n",
      "    \"70\":\n",
      "            \"yes\"\n",
      "    \"68\":\n",
      "            \"yes\"\n",
      "    \"65\":\n",
      "            \"no\"\n",
      "    \"64\":\n",
      "            \"yes\"\n",
      "    \"72\":\n",
      "        \"Outlook\":\n",
      "            \"sunny\":\n",
      "                    \"no\"\n",
      "            \"overcast\":\n",
      "                    \"yes\"\n",
      "    \"69\":\n",
      "            \"yes\"\n",
      "    \"75\":\n",
      "            \"yes\"\n",
      "    \"81\":\n",
      "            \"yes\"\n",
      "    \"71\":\n",
      "            \"no\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def format_json(json_data, indent_level=0):\n",
    "    formatted_json = ''\n",
    "    \n",
    "    if isinstance(json_data, dict):\n",
    "        for key, value in json_data.items():\n",
    "            formatted_json += ' ' * indent_level + f'\"{key}\":\\n'\n",
    "            formatted_json += format_json(value, indent_level + 4)\n",
    "    elif isinstance(json_data, list):\n",
    "        for item in json_data:\n",
    "            formatted_json += ' ' * indent_level + '-\\n'\n",
    "            formatted_json += format_json(item, indent_level + 4)\n",
    "    else:\n",
    "        formatted_json += ' ' * (indent_level + 4) + json.dumps(json_data) + '\\n'\n",
    "    \n",
    "    return formatted_json\n",
    "\n",
    "# Example JSON data\n",
    "\n",
    "formatted_json = format_json(c45_decision_tree)\n",
    "print(formatted_json)\n",
    "formatted_json = format_json(decision_tree)\n",
    "print(formatted_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.287054028118727\n",
      "0.7506415278096601\n",
      "Temp\n",
      "0    No\n",
      "Name: Decision, dtype: object\n",
      "1    No\n",
      "Name: Decision, dtype: object\n",
      "2    yes\n",
      "Name: Decision, dtype: object\n",
      "3    yes\n",
      "Name: Decision, dtype: object\n",
      "4    yes\n",
      "Name: Decision, dtype: object\n",
      "5    no\n",
      "Name: Decision, dtype: object\n",
      "6    yes\n",
      "Name: Decision, dtype: object\n",
      "7    no\n",
      "Name: Decision, dtype: object\n",
      "11    yes\n",
      "Name: Decision, dtype: object\n",
      "8    yes\n",
      "Name: Decision, dtype: object\n",
      "9     yes\n",
      "10    yes\n",
      "Name: Decision, dtype: object\n",
      "12    yes\n",
      "Name: Decision, dtype: object\n",
      "13    no\n",
      "Name: Decision, dtype: object\n",
      "Decision Tree:\n",
      "{'Temp': {85: 'No', 80: 'No', 83: 'yes', 70: 'yes', 68: 'yes', 65: 'no', 64: 'yes', 72: {'Outlook': {'sunny': 'no', 'overcast': 'yes'}}, 69: 'yes', 75: 'yes', 81: 'yes', 71: 'no'}}\n"
     ]
    }
   ],
   "source": [
    "def entorpy(data):\n",
    "    labels = data['Decision']\n",
    "    total = len(labels)\n",
    "    value_count = Counter(labels)\n",
    "    entorpy = 0\n",
    "    for value in value_count:\n",
    "        probablity = value_count[value] / total\n",
    "        entropy -= probablity * np.log2(probablity)\n",
    "    return entropy\n",
    "print(entropy(data))\n",
    "\n",
    "\n",
    "def info_gain(data, attribute):\n",
    "    entopy_before = entropy(data)\n",
    "    unique_counts = data[attribute].unique()\n",
    "    total = len(data[attribute])\n",
    "    weighter_entropy = 0\n",
    "    \n",
    "    for value in unique_counts:\n",
    "        subset = data[data[attribute] == value]\n",
    "        prob = len(subset) / total\n",
    "        weighter_entropy += prob * entropy(subset)\n",
    "    return entopy_before - weighter_entropy\n",
    "print(info_gain(data, \"Humidity\"))\n",
    "        \n",
    "def select_best_attribute(data, attributes):\n",
    "    info_gains = [(attribute, info_gain(data, attribute)) for attribute in attributes]\n",
    "    best_attr, best_gain = max(info_gains, key=lambda x: x[1])\n",
    "    return best_attr\n",
    "print(select_best_attribute(data, [\"Humidity\",\"Outlook\", \"Temp\"]))\n",
    "\n",
    "def id3(data,attributes, parent_data = None):\n",
    "    if len(np.unique(data['Decision'])) <= 1:\n",
    "        print(data['Decision'])\n",
    "        return np.unique(data['Decision'])[0]\n",
    "\n",
    "    # If the dataset is empty, return the classification of the majority class in the parent dataset\n",
    "    elif len(data) == 0:\n",
    "        return np.unique(parent_data['Decision'])[np.argmax(np.unique(parent_data['Decision'], return_counts=True)[1])]\n",
    "\n",
    "    # If there are no more attributes to split on, return the classification of the majority class\n",
    "    elif len(attributes) == 0:\n",
    "        \n",
    "        return np.unique(data['Decision'])[np.argmax(np.unique(data['Decision'], return_counts=True)[1])]\n",
    "    else:\n",
    "        best_attribute = select_best_attribute(data,attributes)\n",
    "        tree = {best_attribute: {}} \n",
    "\n",
    "        for value in data[best_attribute].unique():\n",
    "            sub_data = data[data[best_attribute]==value]\n",
    "            sub_attr = [attr for attr in attributes if attr != best_attribute]\n",
    "\n",
    "            subtree = id3(sub_data,sub_attr, data)\n",
    "            tree[best_attribute][value] = subtree\n",
    "#             print(\"\\ndata\\n\",sub_data,\"\\nsubattr\\n\",sub_attr)\n",
    "        return tree\n",
    "        \n",
    "\n",
    "data = pd.read_csv('DecisionTree.csv')\n",
    "attributes = data.columns[1:-1]  # Exclude the 'Decision' column\n",
    "decision_tree = id3(data, attributes)\n",
    "\n",
    "# Print the decision tree\n",
    "print(\"Decision Tree:\")\n",
    "print(decision_tree)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, Outlook, Temp, Humidity, Wind, Decision]\n",
      "Index: []\n",
      "[85 90 78 96 80 70 65 95 75]\n"
     ]
    }
   ],
   "source": [
    "print(data[data[\"Humidity\"]==72])\n",
    "print(data[\"Humidity\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data\n",
      "    Unnamed: 0 Outlook  Temp  Humidity  Wind Decision\n",
      "0           0   sunny    85        85  weak       No \n",
      "subattr\n",
      " ['Outlook', 'Humidity', 'Wind']\n",
      "\n",
      "data\n",
      "    Unnamed: 0 Outlook  Temp  Humidity    Wind Decision\n",
      "1           1   sunny    80        90  strong       No \n",
      "subattr\n",
      " ['Outlook', 'Humidity', 'Wind']\n",
      "\n",
      "data\n",
      "    Unnamed: 0   Outlook  Temp  Humidity  Wind Decision\n",
      "2           2  overcast    83        78  weak      yes \n",
      "subattr\n",
      " ['Outlook', 'Humidity', 'Wind']\n",
      "\n",
      "data\n",
      "    Unnamed: 0 Outlook  Temp  Humidity  Wind Decision\n",
      "3           3   rainy    70        96  weak      yes \n",
      "subattr\n",
      " ['Outlook', 'Humidity', 'Wind']\n",
      "\n",
      "data\n",
      "    Unnamed: 0 Outlook  Temp  Humidity  Wind Decision\n",
      "4           4   rainy    68        80  weak      yes \n",
      "subattr\n",
      " ['Outlook', 'Humidity', 'Wind']\n",
      "\n",
      "data\n",
      "    Unnamed: 0 Outlook  Temp  Humidity    Wind Decision\n",
      "5           5   rainy    65        70  strong       no \n",
      "subattr\n",
      " ['Outlook', 'Humidity', 'Wind']\n",
      "\n",
      "data\n",
      "    Unnamed: 0   Outlook  Temp  Humidity    Wind Decision\n",
      "6           6  overcast    64        65  strong      yes \n",
      "subattr\n",
      " ['Outlook', 'Humidity', 'Wind']\n",
      "\n",
      "data\n",
      "    Unnamed: 0 Outlook  Temp  Humidity  Wind Decision\n",
      "7           7   sunny    72        95  weak       no \n",
      "subattr\n",
      " ['Humidity', 'Wind']\n",
      "\n",
      "data\n",
      "     Unnamed: 0   Outlook  Temp  Humidity    Wind Decision\n",
      "11          11  overcast    72        90  strong      yes \n",
      "subattr\n",
      " ['Humidity', 'Wind']\n",
      "\n",
      "data\n",
      "     Unnamed: 0   Outlook  Temp  Humidity    Wind Decision\n",
      "7            7     sunny    72        95    weak       no\n",
      "11          11  overcast    72        90  strong      yes \n",
      "subattr\n",
      " ['Outlook', 'Humidity', 'Wind']\n",
      "\n",
      "data\n",
      "    Unnamed: 0 Outlook  Temp  Humidity  Wind Decision\n",
      "8           8   sunny    69        70  weak      yes \n",
      "subattr\n",
      " ['Outlook', 'Humidity', 'Wind']\n",
      "\n",
      "data\n",
      "     Unnamed: 0 Outlook  Temp  Humidity    Wind Decision\n",
      "9            9   rainy    75        80    weak      yes\n",
      "10          10   sunny    75        70  strong      yes \n",
      "subattr\n",
      " ['Outlook', 'Humidity', 'Wind']\n",
      "\n",
      "data\n",
      "     Unnamed: 0   Outlook  Temp  Humidity  Wind Decision\n",
      "12          12  overcast    81        75  weak      yes \n",
      "subattr\n",
      " ['Outlook', 'Humidity', 'Wind']\n",
      "\n",
      "data\n",
      "     Unnamed: 0 Outlook  Temp  Humidity    Wind Decision\n",
      "13          13   rainy    71        80  strong       no \n",
      "subattr\n",
      " ['Outlook', 'Humidity', 'Wind']\n",
      "Decision Tree:\n",
      "{'Temp': {85: 'No', 80: 'No', 83: 'yes', 70: 'yes', 68: 'yes', 65: 'no', 64: 'yes', 72: {'Outlook': {'sunny': 'no', 'overcast': 'yes'}}, 69: 'yes', 75: 'yes', 81: 'yes', 71: 'no'}}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# Define the entropy function\n",
    "def entropy(data):\n",
    "    labels = data['Decision']\n",
    "    total = len(labels)\n",
    "    value_count = Counter(labels)\n",
    "    entropy = 0  # Fixed variable name typo: entropy\n",
    "    for value in value_count:\n",
    "        probability = value_count[value] / total\n",
    "        entropy -= probability * math.log2(probability)  # Used math.log2 instead of np.log2\n",
    "    return entropy\n",
    "\n",
    "# Define the info_gain function\n",
    "def info_gain(data, attribute):\n",
    "    entropy_before = entropy(data)  # Fixed variable name typo: entropy_before\n",
    "    unique_counts = data[attribute].unique()\n",
    "    total = len(data[attribute])\n",
    "    weighted_entropy = 0  # Fixed variable name typo: weighted_entropy\n",
    "    \n",
    "    for value in unique_counts:\n",
    "        subset = data[data[attribute] == value]\n",
    "        prob = len(subset) / total\n",
    "        weighted_entropy += prob * entropy(subset)\n",
    "    return entropy_before - weighted_entropy\n",
    "\n",
    "# Define the select_best_attribute function\n",
    "def select_best_attribute(data, attributes):\n",
    "    info_gains = [(attribute, info_gain(data, attribute)) for attribute in attributes]\n",
    "    best_attr, best_gain = max(info_gains, key=lambda x: x[1])\n",
    "    return best_attr\n",
    "\n",
    "# Define the id3 function\n",
    "def id3(data, attributes, parent_data=None):\n",
    "    if len(np.unique(data['Decision'])) <= 1:\n",
    "        return np.unique(data['Decision'])[0]\n",
    "\n",
    "    elif len(data) == 0:\n",
    "        return np.unique(parent_data['Decision'])[np.argmax(np.unique(parent_data['Decision'], return_counts=True)[1])]\n",
    "\n",
    "    elif len(attributes) == 0:\n",
    "        return np.unique(data['Decision'])[np.argmax(np.unique(data['Decision'], return_counts=True)[1])]\n",
    "                                           \n",
    "    else:\n",
    "        best_attribute = select_best_attribute(data, attributes)\n",
    "        tree = {best_attribute: {}}\n",
    "\n",
    "        for value in data[best_attribute].unique():\n",
    "            sub_data = data[data[best_attribute] == value]\n",
    "            sub_attr = [attr for attr in attributes if attr != best_attribute]\n",
    "\n",
    "            subtree = id3(sub_data, sub_attr, data)\n",
    "            tree[best_attribute][value] = subtree\n",
    "            print(\"\\ndata\\n\",sub_data,\"\\nsubattr\\n\",sub_attr)\n",
    "        return tree\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('DecisionTree.csv')\n",
    "attributes = data.columns[1:-1]  # Exclude the 'Decision' column\n",
    "decision_tree = id3(data, attributes)\n",
    "\n",
    "# Print the decision tree\n",
    "print(\"Decision Tree:\")\n",
    "print(decision_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-e4ec0408f67f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Experience'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Salary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1369\u001b[0m         )\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m             )\n\u001b[1;32m    649\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data.csv\")\n",
    "x = df['Experience']\n",
    "y = df['Salary']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
